{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fbf5e1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework, we have worked with the given dataset and we have tried some improvements to the algorithm presented during the course.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "To test our algorithms, you first need to have the data in the same folder as this notebook. Otherwise, you can change the variable <code>data_path</code> in the case below [Import data](#Import-data) to make it convinient for your usage. \n",
    "<br>\n",
    "<br>\n",
    "You only need to run this notebook to test our algorithms. The different libraries used are <code>numpy</code>, <code>itertools</code>, <code>math</code>, and <code>time</code>. So you have to install them on your environment before launching the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce84d3",
   "metadata": {},
   "source": [
    "\n",
    "# Technical report\n",
    "The different steps of our work are : \n",
    "<ul>\n",
    "    <li><u>Importation and save of data :</u> We have collected the data in a list <code>data</code>. Each item of the list correspond to a different basket which is represented by a frozenset. <a href=\"#Import-data\"> [Link]</a></li>\n",
    "    <li><u>Implementation of Apriori algorithm :</u> The different functions were computed.<a href=\"#A-priori-algorithm\"> [Link]</a>\n",
    "        <ul>\n",
    "            <li><u>Implementation of the <code>get_candidate</code> function :</u> To get the candidate at a level $l$, the method presented in the course is to make all the combinations between the frequent itemsets found at the level $l-1$ and the frequent item from level $1$. We only have to be careful not to add a singleton that is already in the analyzed itemset.</li>\n",
    "            <li><u>Implementation of the <code>get_candidate_2</code> function :</u> The paper gives another proprosition to implement this function. Instead of adding the singletons, it purposes to combine frequent itemsets of the level $l-1$ one with another and keep only those which unions are level $l$. To do so, we have computed the length of the different of the itemsets. If the difference is $1$. That means that the union is a candidate since it is of size $l$.</li>\n",
    "            <li><u>Implementation of the <code>get_candidate_2</code> function :</u> The paper gives another proprosition to implement this function. Instead of adding the singletons, it purposes to combine frequent itemsets of the level $l-1$ one with another and keep only those which unions are level $l$. To do so, we have computed the length of the different of the itemsets. If the difference is $1$. That means that the union is a candidate since it is of size $l$.</li>\n",
    "            <li><u>Implementation of the <code>get_frequent_combination</code> function :</u> This function follows the idea of the course. We read all baskets. In each basket, we create all combinations of size $l$. If a combination is among the candidates, we count it. It requires to create a map between the candidates and the table <code>count</code>. It is done by the dictionary <code>dict_set</code>.</li>\n",
    "            <li><u>Implementation of the <code>get_frequent_candidate</code> function :</u> We have thought to an alternative of the previous algorithm. Instead of computing all combinations (that implies a lot of loops), we can make the loop on the candidates if their amount are small enough. That means that we look for all candidates in every basket instead looking for all combinations in the candidates.</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li><u>Implementation of genrating rules :</u> To generate rules, we start from a set of frequent item(set)s in the basket. We read once all the basket and count the occurence of each product (if not included in the frequent itemset) when each frequent item(set) is present. We also need to count the support of each frequent item(set). Once it is done, we only look for each frequent item(set), the frequent items with $\\frac{\\#_{item \\cap  itemset}}{\\#_{itemset}}$ confidence. We limit ourself to results of the rule of size 1, in the first part. In a second part, we implement an algorithm similar as the Apriori algorithm to find rules with several items in the implication.<a href=\"#Generating-rules\"> [Link]</a></li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0448995",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739dd5c",
   "metadata": {},
   "source": [
    "## Apriori Algorithm\n",
    "We can compare the performance of the different algorithm. First in term of computation time (time are given in seconds):\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Step</th>\n",
    "        <th>Method 1</th>\n",
    "        <th>Method 2</th>\n",
    "        <th>Method 3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Functions</th>\n",
    "        <td><code>get_candidate</code> and <code>get_frequent_combination</code></td>\n",
    "        <td><code>get_candidate_2</code> and <code>get_frequent_combination</code></td>\n",
    "        <td><code>get_candidate_2</code> and mix of <code>get_frequent_combination</code> and <code>get_frequent_candidate</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Candidates for singletons</th>\n",
    "        <td>0.32</td>\n",
    "        <td>0.32</td>\n",
    "        <td>0.34</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Find frequent singletons</th>\n",
    "        <td>0.94</td>\n",
    "        <td>1.01</td>\n",
    "        <td>0.93</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Candidates for doubletons</th>\n",
    "        <td>0.22</td>\n",
    "        <td>0.05</td>\n",
    "        <td>0.06</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Find frequent doubletons</th>\n",
    "        <td>4.54</td>\n",
    "        <td>4.85</td>\n",
    "        <td>4.87</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Candidates for tripletons</th>\n",
    "        <td>0.01</td>\n",
    "        <td>0.01</td>\n",
    "        <td>0.01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Find frequent tripletons</th>\n",
    "        <td>4.74</td>\n",
    "        <td>5.11</td>\n",
    "        <td>0.06</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Candidates for quatripletons</th>\n",
    "        <td>0.0</td>\n",
    "        <td>0.0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Find frequent quatripletons</th>\n",
    "        <td>13.02</td>\n",
    "        <td>13.42</td>\n",
    "        <td>0.04</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th scope=\"row\">Total</th>\n",
    "        <td>23.79</td>\n",
    "        <td>24.76</td>\n",
    "        <td>13.02</td>\n",
    "    </tr>\n",
    "</table>\n",
    "Then in term of the size of candidates:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th><code>get_candidate</code></th>\n",
    "        <th><code>get_candidate_2</code></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Number of candidate of singletons</td>\n",
    "        <td>870</td>\n",
    "        <td>870</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Number of candidate of doubletons</td>\n",
    "        <td>70125</td>\n",
    "        <td>70125</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Number of candidate of tripletons</td>\n",
    "        <td>3352</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Number of candidate of quadrupletons</td>\n",
    "        <td>372</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959a0e4",
   "metadata": {},
   "source": [
    "We can deduce that the new method to generate candidate is a lot more efficient in terms of number of candidates but it decreases slightly the computation time if we do not modify the other algorithm. If we add some improvements to the initial method, we can save a lot of time of computation.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Note: there will be not effect between method 1 and a method with <code>get_candidate</code> and a mix of <code>get_frequent_combination</code> and <code>get_frequent_candidate</code> since the number of candidate would be higher than the number of possible combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f1545",
   "metadata": {},
   "source": [
    "## Generating rules\n",
    "All the rules that we have created in both models Rules and Rules_Multi are verified. We have obtained 51 rules in the first model and 120 in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736e36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34e692",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff8f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data.dat\"\n",
    "data= []\n",
    "with open(data_path, encoding=\"utf-8\") as f:\n",
    "        tab = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b3aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in (tab):\n",
    "    tab_dat = a.split(\" \\n\")[0].split(\" \")\n",
    "    set_sample = set()\n",
    "    for val in tab_dat:\n",
    "        set_sample.add(int(val))\n",
    "    data.append(frozenset(set_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab65193",
   "metadata": {},
   "source": [
    "# A-priori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3042ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_support = 0.01       #proportion\n",
    "threshold_confidence = 0.5     #proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c3fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_singletons(data):\n",
    "    res = set()\n",
    "    for basket in data:\n",
    "        for item in basket:\n",
    "            res.add(frozenset({item}))\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_candidates(frequent_rank_1, frequent_rank_k):\n",
    "    res = set()\n",
    "    for itemset in frequent_rank_k:\n",
    "        for item in frequent_rank_1:\n",
    "            if(not item.issubset(itemset)):\n",
    "                res.add(frozenset(item.union(itemset)))\n",
    "    return res\n",
    "\n",
    "def get_candidates_2(frequent_rank_k):\n",
    "    res = set()\n",
    "    for i,itemset_1 in enumerate(frequent_rank_k):\n",
    "        for itemset_2 in frequent_rank_k[:i]:\n",
    "            if(len(itemset_1.difference(itemset_2))==1):\n",
    "                res.add(frozenset(itemset_1.union(itemset_2)))\n",
    "    return res\n",
    "\n",
    "def get_frequent_combination(data, candidate, support, level, message=False):\n",
    "    if(message):\n",
    "        print(\"Usage of get_frequent_combination function\")\n",
    "    count = np.zeros(len(candidate))\n",
    "    dict_set = {frozen : i for i,frozen in enumerate(list(candidate))}\n",
    "    for basket in data:\n",
    "        comb = combinations(basket, level)\n",
    "        for possible in comb:\n",
    "            if (frozenset(possible) in candidate):\n",
    "                count[dict_set[frozenset(possible)]]+=1\n",
    "    return [cand for (cand, a) in zip(candidate, count) if a > support]\n",
    "\n",
    "def get_frequent_candidate(data, candidate, support, level, message=False):\n",
    "    if(message):\n",
    "        print(\"Usage of get_frequent_candidate function\")\n",
    "    count = np.zeros(len(candidate))\n",
    "    for basket in data:\n",
    "        for i,cand in enumerate(list(candidate)):\n",
    "            if (cand.issubset(basket)):\n",
    "                count[i]+=1\n",
    "    return [cand for (cand, a) in zip(candidate, count) if a > support]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41577cc8",
   "metadata": {},
   "source": [
    "## Method 1 : Course algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8498f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 870 candidates for 1-tons \t Computation time : 0.29 s\n",
      "We have found 375 frequent 1-tons\t\t Computation time : 0.87 s\n",
      "\n",
      "\n",
      "We have found 70125 candidates for 2-tons \t Computation time : 0.13 s\n",
      "We have found 9 frequent 2-tons\t\t Computation time : 4.26 s\n",
      "\n",
      "\n",
      "We have found 3352 candidates for 3-tons \t Computation time : 0.01 s\n",
      "We have found 1 frequent 3-tons\t\t Computation time : 5.0 s\n",
      "\n",
      "\n",
      "We have found 372 candidates for 4-tons \t Computation time : 0.0 s\n",
      "We have found 0 frequent 4-tons\t\t Computation time : 12.93 s\n",
      "\n",
      "\n",
      "Global computation time : 23.49 s\n"
     ]
    }
   ],
   "source": [
    "freq =[]\n",
    "length_last = 1\n",
    "level = 0\n",
    "t_0=time.time()\n",
    "\n",
    "while (length_last>0):\n",
    "    level+=1\n",
    "    time_init = time.time()\n",
    "    if (level==1):\n",
    "        candidates = get_candidates_singletons(data)\n",
    "        candidates_1 = candidates.copy()\n",
    "    else:\n",
    "        candidates = get_candidates(freq[0],freq[level-2])\n",
    "    print(\"We have found\", len(candidates),\"candidates for \"+str(level)+\"-tons \\t Computation time :\",round(time.time()-time_init,2),\"s\")\n",
    "    time_init = time.time()\n",
    "    freq.append(get_frequent_combination(data,candidates,threshold_support*len(data), level))\n",
    "    print(\"We have found\",len(freq[level-1]),\"frequent \"+str(level)+\"-tons\\t\\t Computation time :\",round(time.time()-time_init,2),\"s\\n\\n\")\n",
    "    length_last = len(freq[level-1])\n",
    "print(\"Global computation time :\", round(time.time()-t_0,2),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f8e46",
   "metadata": {},
   "source": [
    "## Method 2 : Paper algorithm for get_candidate & Course algorithm for get_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daca48",
   "metadata": {},
   "source": [
    "The difference is that the course purposes to add values of $frequent_1$ to $frequent_k$ whereas the paper purposes to directly compute from $frequent_k$ and look for combination that are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fe6653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 870 candidates for 1-tons \t Computation time : 0.33 s\n",
      "We have found 375 frequent 1-tons\t\t Computation time : 0.94 s\n",
      "\n",
      "\n",
      "We have found 70125 candidates for 2-tons \t Computation time : 0.06 s\n",
      "We have found 9 frequent 2-tons\t\t Computation time : 4.5 s\n",
      "\n",
      "\n",
      "We have found 4 candidates for 3-tons \t Computation time : 0.01 s\n",
      "We have found 1 frequent 3-tons\t\t Computation time : 4.6 s\n",
      "\n",
      "\n",
      "We have found 0 candidates for 4-tons \t Computation time : 0.0 s\n",
      "We have found 0 frequent 4-tons\t\t Computation time : 12.47 s\n",
      "\n",
      "\n",
      "Global computation time : 22.92 s\n"
     ]
    }
   ],
   "source": [
    "freq =[]\n",
    "length_last = 10000000\n",
    "level = 0\n",
    "t_0= time.time()\n",
    "\n",
    "while (length_last>0):\n",
    "    level+=1\n",
    "    time_init = time.time()\n",
    "    if (level==1):\n",
    "        candidates = get_candidates_singletons(data)\n",
    "        candidates_1 = candidates.copy()\n",
    "    else:\n",
    "        candidates = get_candidates_2(freq[level-2])\n",
    "    print(\"We have found\", len(candidates),\"candidates for \"+str(level)+\"-tons \\t Computation time :\",round(time.time()-time_init,2),\"s\")\n",
    "    time_init = time.time()\n",
    "    freq.append(get_frequent_combination(data,candidates,threshold_support*len(data), level))\n",
    "    print(\"We have found\",len(freq[level-1]),\"frequent \"+str(level)+\"-tons\\t\\t Computation time :\",round(time.time()-time_init,2),\"s\\n\\n\")\n",
    "    length_last = len(freq[level-1])\n",
    "print(\"Global computation time :\", round(time.time()-t_0,2),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d11e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 870 candidates for 1-tons \t Computation time : 0.33 s\n",
      "We have found 375 frequent 1-tons\t\t Computation time : 0.94 s\n",
      "\n",
      "\n",
      "We have found 70125 candidates for 2-tons \t Computation time : 0.1 s\n",
      "We have found 9 frequent 2-tons\t\t Computation time : 4.51 s\n",
      "\n",
      "\n",
      "We have found 4 candidates for 3-tons \t Computation time : 0.01 s\n",
      "We have found 1 frequent 3-tons\t\t Computation time : 4.79 s\n",
      "\n",
      "\n",
      "We have found 0 candidates for 4-tons \t Computation time : 0.0 s\n",
      "Global computation time : 10.67 s\n"
     ]
    }
   ],
   "source": [
    "freq =[]\n",
    "length_last = 10000000\n",
    "level = 0\n",
    "t_0= time.time()\n",
    "\n",
    "while (length_last>0):\n",
    "    level+=1\n",
    "    time_init = time.time()\n",
    "    if (level==1):\n",
    "        candidates = get_candidates_singletons(data)\n",
    "        candidates_1 = candidates.copy()\n",
    "    else:\n",
    "        candidates = get_candidates_2(freq[level-2])\n",
    "    print(\"We have found\", len(candidates),\"candidates for \"+str(level)+\"-tons \\t Computation time :\",round(time.time()-time_init,2),\"s\")\n",
    "    time_init = time.time()\n",
    "    if(len(candidates)>0):\n",
    "        freq.append(get_frequent_combination(data,candidates,threshold_support*len(data), level))\n",
    "        print(\"We have found\",len(freq[level-1]),\"frequent \"+str(level)+\"-tons\\t\\t Computation time :\",round(time.time()-time_init,2),\"s\\n\\n\")\n",
    "        length_last = len(freq[level-1])\n",
    "    else:\n",
    "        length_last = 0\n",
    "print(\"Global computation time :\", round(time.time()-t_0,2),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1e72d",
   "metadata": {},
   "source": [
    "### Method 3 : Paper algorithm for get_candidate and Home algorithm for get_frequent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821ce3",
   "metadata": {},
   "source": [
    "The idea is to exchange the loop. Instead of doing a loop on the number of combinations (which can be very high $\\binom{n}{k}\\times N_{basket}$ where $n$ is the average size of a basket). It is possible to do it on the candidates what generates $n_{candidates}\\times N_{basket}$ loops. It is relevant when $n_{candidates}<\\binom{n}{k}$.\n",
    "<br>\n",
    "<br>\n",
    "As we can see in the previous computations, the algorithm provided by the Paper allows to decrease signficantly the number of candidates. So it could be interessant to combine both ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e7b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 870 candidates for 1-tons \t Computation time : 0.33 s\n",
      "Usage of get_frequent_combination function\n",
      "We have found 375 frequent 1-tons\t\t Computation time : 0.98 s\n",
      "\n",
      "\n",
      "We have found 70125 candidates for 2-tons \t Computation time : 0.13 s\n",
      "Usage of get_frequent_combination function\n",
      "We have found 9 frequent 2-tons\t\t Computation time : 4.75 s\n",
      "\n",
      "\n",
      "We have found 4 candidates for 3-tons \t Computation time : 0.01 s\n",
      "Usage of get_frequent_candidate function\n",
      "We have found 1 frequent 3-tons\t\t Computation time : 0.06 s\n",
      "\n",
      "\n",
      "We have found 0 candidates for 4-tons \t Computation time : 0.0 s\n",
      "Usage of get_frequent_candidate function\n",
      "We have found 0 frequent 4-tons\t\t Computation time : 0.03 s\n",
      "\n",
      "\n",
      "Global computation time : 6.32 s\n"
     ]
    }
   ],
   "source": [
    "freq =[]\n",
    "length_last = 10000000\n",
    "level = 0\n",
    "t_0 = time.time()\n",
    "average_size = int(np.mean([len(basket) for basket in data]))\n",
    "\n",
    "while (length_last>0):\n",
    "    level+=1\n",
    "    time_init = time.time()\n",
    "    if (level==1):\n",
    "        candidates = get_candidates_singletons(data)\n",
    "        candidates_1 = candidates.copy()\n",
    "    else:\n",
    "        candidates = get_candidates_2(freq[level-2])\n",
    "    print(\"We have found\", len(candidates),\"candidates for \"+str(level)+\"-tons \\t Computation time :\",round(time.time()-time_init,2),\"s\")\n",
    "    time_init = time.time()\n",
    "    if(math.comb(average_size,level)<len(candidates)):\n",
    "        freq.append(get_frequent_combination(data,candidates,threshold_support*len(data), level, message=True))\n",
    "    else:\n",
    "        freq.append(get_frequent_candidate(data,candidates,threshold_support*len(data), level, message=True))\n",
    "    print(\"We have found\",len(freq[level-1]),\"frequent \"+str(level)+\"-tons\\t\\t Computation time :\",round(time.time()-time_init,2),\"s\\n\\n\")\n",
    "    length_last = len(freq[level-1])\n",
    "print(\"Global computation time :\", round(time.time()-t_0,2),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "724b6a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 870 candidates for 1-tons \t Computation time : 0.33 s\n",
      "Usage of get_frequent_combination function\n",
      "We have found 375 frequent 1-tons\t\t Computation time : 0.96 s\n",
      "\n",
      "\n",
      "We have found 70125 candidates for 2-tons \t Computation time : 0.12 s\n",
      "Usage of get_frequent_combination function\n",
      "We have found 9 frequent 2-tons\t\t Computation time : 5.03 s\n",
      "\n",
      "\n",
      "We have found 4 candidates for 3-tons \t Computation time : 0.01 s\n",
      "Usage of get_frequent_candidate function\n",
      "We have found 1 frequent 3-tons\t\t Computation time : 0.07 s\n",
      "\n",
      "\n",
      "We have found 0 candidates for 4-tons \t Computation time : 0.0 s\n",
      "Global computation time : 6.53 s\n"
     ]
    }
   ],
   "source": [
    "freq =[]\n",
    "length_last = 10000000\n",
    "level = 0\n",
    "t_0 = time.time()\n",
    "average_size = int(np.mean([len(basket) for basket in data]))\n",
    "\n",
    "while (length_last>0):\n",
    "    level+=1\n",
    "    time_init = time.time()\n",
    "    if (level==1):\n",
    "        candidates = get_candidates_singletons(data)\n",
    "        candidates_1 = candidates.copy()\n",
    "    else:\n",
    "        candidates = get_candidates_2(freq[level-2])\n",
    "    print(\"We have found\", len(candidates),\"candidates for \"+str(level)+\"-tons \\t Computation time :\",round(time.time()-time_init,2),\"s\")\n",
    "    time_init = time.time()\n",
    "    if(len(candidates)>0):\n",
    "        if(math.comb(average_size,level)<len(candidates)):\n",
    "            freq.append(get_frequent_combination(data,candidates,threshold_support*len(data), level, message=True))\n",
    "        else:\n",
    "            freq.append(get_frequent_candidate(data,candidates,threshold_support*len(data), level, message=True))\n",
    "        print(\"We have found\",len(freq[level-1]),\"frequent \"+str(level)+\"-tons\\t\\t Computation time :\",round(time.time()-time_init,2),\"s\\n\\n\")\n",
    "        length_last = len(freq[level-1])\n",
    "    else:\n",
    "        length_last = 0\n",
    "print(\"Global computation time :\", round(time.time()-t_0,2),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fac5e",
   "metadata": {},
   "source": [
    "# Generating rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b977187",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = set(np.concatenate(freq))\n",
    "map_dict = {itemset: i for i,itemset in enumerate(frequent_itemsets)}\n",
    "map_item = {item : i for i,item in enumerate(candidates_1)}\n",
    "reversed_map_item = {i: item for item, i in map_item.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321d9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, origin, result,threshold_confidence):\n",
    "        self.origin = origin\n",
    "        self.implication = result\n",
    "        self.threshold_confidence = threshold_confidence\n",
    "        \n",
    "    def show(self):\n",
    "        freq_item = list(self.origin)\n",
    "        res = list(self.implication)\n",
    "        print(\"{\"+list_to_string(list(self.origin))+\"}->{\"+list_to_string(list(self.implication))+\"}\")\n",
    "\n",
    "    def equal(self,Rule):\n",
    "        return (self.origin==Rule.origin)&(self.implication==Rule.implication)&(self.threshold_confidence==Rule.threshold_confidence)\n",
    "    \n",
    "    def verify(self, data, threshold_support):\n",
    "        count_origin=0\n",
    "        count=0\n",
    "        for basket in data:\n",
    "            if self.origin.issubset(basket):\n",
    "                count_origin+=1\n",
    "                if self.implication.issubset(basket):\n",
    "                    count+=1\n",
    "        return (count>count_origin*self.threshold_confidence)&(count_origin>len(data)*threshold_support)\n",
    "\n",
    "class Rules :\n",
    "\n",
    "    def __init__(self,data,candidates_1,frequent_itemsets, threshold_confidence):\n",
    "        count = np.zeros((len(frequent_itemsets),len(candidates_1)))\n",
    "        count_freq = np.zeros(len(frequent_itemsets))\n",
    "        for basket in data:\n",
    "            for frequent_itemset in frequent_itemsets:\n",
    "                if frequent_itemset.issubset(basket):\n",
    "                    count_freq[map_dict[frequent_itemset]]+=1\n",
    "                    for item in basket.difference(frequent_itemset):\n",
    "                        count[map_dict[frequent_itemset]][map_item[frozenset({item})]]+=1\n",
    "        self.rules = set()\n",
    "        for i,frequent_itemset in enumerate(frequent_itemsets):\n",
    "            r = [Rule(frequent_itemset,reversed_map_item[j],threshold_confidence) for j,a in enumerate(count[i]) if threshold_confidence<a/count_freq[i]]\n",
    "            for ru in r:\n",
    "                self.rules.add(ru)\n",
    "                \n",
    "    def verify(self,data,support):\n",
    "        for rule in self.rules:\n",
    "                if (not rule.verify(data,support)):\n",
    "                    return False\n",
    "        return True\n",
    "                \n",
    "    def show(self):\n",
    "        for rule in self.rules :\n",
    "            rule.show()\n",
    "        \n",
    "    def isinModel(self,Rule):\n",
    "        for rule in self.rules:\n",
    "            if(rule.equal(Rule)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def list_to_string(l):\n",
    "    res=\"\"\n",
    "    for a in l:\n",
    "        res+=str(a)+\",\"\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ba6b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51  rules have been found.\n"
     ]
    }
   ],
   "source": [
    "Rules_res = Rules(data,candidates_1,frequent_itemsets, threshold_confidence)\n",
    "print(len(Rules_res.rules),\" rules have been found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bab231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{546}->{923}\n",
      "{208}->{969}\n",
      "{458}->{888}\n",
      "{678}->{471}\n",
      "{978}->{883}\n",
      "{208}->{888}\n",
      "{546}->{217}\n",
      "{546}->{661}\n",
      "{688}->{132}\n",
      "{33}->{515}\n",
      "{366}->{183}\n",
      "{515}->{346}\n",
      "{819}->{765}\n",
      "{105}->{815}\n",
      "{825,39}->{704}\n",
      "{704}->{825}\n",
      "{105}->{944}\n",
      "{708}->{978}\n",
      "{448}->{538}\n",
      "{227,390}->{12}\n",
      "{33}->{217}\n",
      "{227}->{722}\n",
      "{33}->{346}\n",
      "{722,390}->{227}\n",
      "{227,390}->{722}\n",
      "{217,346}->{283}\n",
      "{515}->{283}\n",
      "{515}->{217}\n",
      "{227}->{390}\n",
      "{105}->{862}\n",
      "{208}->{290}\n",
      "{33}->{283}\n",
      "{458}->{969}\n",
      "{546}->{947}\n",
      "{217,346}->{515}\n",
      "{458}->{208}\n",
      "{208}->{458}\n",
      "{496}->{626}\n",
      "{704}->{39}\n",
      "{458}->{290}\n",
      "{819}->{684}\n",
      "{722,390}->{12}\n",
      "{515}->{33}\n",
      "{708}->{883}\n",
      "{704,825}->{39}\n",
      "{819}->{70}\n",
      "{217,346}->{33}\n",
      "{704,39}->{825}\n",
      "{912}->{348}\n",
      "{678}->{960}\n",
      "{105}->{494}\n"
     ]
    }
   ],
   "source": [
    "Rules_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6f52e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rules_res.verify(data,threshold_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e99bdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rules_multi :\n",
    "    def __init__(self,data,candidates_1,frequent_itemsets, threshold_confidence):\n",
    "        count = np.zeros((len(frequent_itemsets),len(candidates_1)))\n",
    "        count_freq = np.zeros(len(frequent_itemsets))\n",
    "        for basket in data:\n",
    "            for frequent_itemset in frequent_itemsets:\n",
    "                if frequent_itemset.issubset(basket):\n",
    "                    count_freq[map_dict[frequent_itemset]]+=1\n",
    "                    for item in basket.difference(frequent_itemset):\n",
    "                        count[map_dict[frequent_itemset]][map_item[frozenset({item})]]+=1\n",
    "        self.rules = set()\n",
    "        self.to_inspect = []\n",
    "        for i,frequent_itemset in enumerate(frequent_itemsets):\n",
    "            r = [Rule(frequent_itemset,reversed_map_item[j],threshold_confidence) for j,a in enumerate(count[i]) if threshold_confidence<a/count_freq[i]]\n",
    "            for ru in r:\n",
    "                self.rules.add(ru)\n",
    "            if len(r)>1:\n",
    "                self.to_inspect.append(r)\n",
    "        self.to_inspect_0 = self.to_inspect.copy()\n",
    "        \n",
    "    \n",
    "    def inspection(self, data, threshold_confidence):\n",
    "        generators = []\n",
    "        values = []\n",
    "        for tab_rule in self.to_inspect:\n",
    "            generators.append(tab_rule[0].origin)\n",
    "            for tab_rule_0 in self.to_inspect:\n",
    "                if(tab_rule[0].origin==tab_rule_0[0].origin):\n",
    "                    a=set()\n",
    "                    for rul_0 in tab_rule_0:\n",
    "                        for rul in tab_rule:\n",
    "                            if(not rul_0.implication.issubset(rul.implication)):\n",
    "                                a.add(frozenset(rul_0.implication.union(rul.implication)))\n",
    "                    break\n",
    "            values.append(a) #val = candidates\n",
    "        self.to_inspect=[]\n",
    "        count = [np.zeros(len(val)) for val in values]\n",
    "        count_freq = np.zeros(len(generators))\n",
    "        map_dict_temp = {itemset: i for i,itemset in enumerate(generators)}\n",
    "        reversed_map_item_temp = [{j : a for j,a in enumerate(val)} for val in (values)]\n",
    "        for basket in data:\n",
    "            for val,frequent_itemset in zip(values,generators):\n",
    "                if frequent_itemset.issubset(basket):\n",
    "                    count_freq[map_dict_temp[frequent_itemset]]+=1\n",
    "                    for j,itemset in enumerate(val):\n",
    "                        if (frozenset(itemset).issubset(basket)):\n",
    "                            count[map_dict_temp[frequent_itemset]][j]+=1\n",
    "        self.to_inspect = []\n",
    "        for i,frequent_itemset in enumerate(generators):\n",
    "            r = [Rule(frequent_itemset,reversed_map_item_temp[i][j],threshold_confidence) for j,a in enumerate(count[i]) if threshold_confidence<a/count_freq[i]]\n",
    "            for ru in r:\n",
    "                self.rules.add(ru)\n",
    "            if len(r)>1:\n",
    "                self.to_inspect.append(r)\n",
    "    \n",
    "    def verify(self,data,support):\n",
    "        for rule in self.rules:\n",
    "                if (not rule.verify(data, support)):\n",
    "                    return False\n",
    "        return True\n",
    "                \n",
    "    def show(self):\n",
    "        for rule in self.rules :\n",
    "            rule.show()    \n",
    "    \n",
    "    def isinModel(self,Rule):\n",
    "        for rule in self.rules:\n",
    "            if(rule.equal(Rule)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def list_to_string(l):\n",
    "    res=\"\"\n",
    "    for a in l:\n",
    "        res+=str(a)+\",\"\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b2d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33}->{515}\n",
      "{33}->{515,217,346,283}\n",
      "{515}->{33,217,346,283}\n",
      "{33}->{283}\n",
      "{105}->{944}\n",
      "{546}->{947,661,217,923}\n",
      "{515}->{217}\n",
      "{515}->{33}\n",
      "{458}->{208,290,888,969}\n",
      "{722,390}->{12}\n",
      "{546}->{661}\n",
      "{688}->{132}\n",
      "{722,390}->{227}\n",
      "{496}->{626}\n",
      "{515}->{283}\n",
      "{515}->{346}\n",
      "{546}->{217}\n",
      "{217,346}->{33}\n",
      "{105}->{494}\n",
      "{227}->{722}\n",
      "{105}->{862}\n",
      "{217,346}->{283}\n",
      "{825,39}->{704}\n",
      "{978}->{883}\n",
      "{458}->{290}\n",
      "{819}->{684}\n",
      "{458}->{969}\n",
      "{708}->{883}\n",
      "{217,346}->{515}\n",
      "{546}->{923}\n",
      "{546}->{947}\n",
      "{704}->{39}\n",
      "{366}->{183}\n",
      "{819}->{765}\n",
      "{704,39}->{825}\n",
      "{912}->{348}\n",
      "{227,390}->{12}\n",
      "{819}->{70}\n",
      "{208}->{290}\n",
      "{33}->{217}\n",
      "{704}->{825}\n",
      "{458}->{888}\n",
      "{33}->{346}\n",
      "{448}->{538}\n",
      "{704,825}->{39}\n",
      "{105}->{815}\n",
      "{708}->{978}\n",
      "{227}->{390}\n",
      "{458}->{208}\n",
      "{227,390}->{722}\n",
      "{819}->{765,70}\n",
      "{546}->{923,661}\n",
      "{458}->{888,290}\n",
      "{515}->{33,217}\n",
      "{515}->{33,283}\n",
      "{458}->{888,969}\n",
      "{515}->{346,283}\n",
      "{458}->{969,290}\n",
      "{458}->{888,208}\n",
      "{546}->{217,947}\n",
      "{458}->{208,290}\n",
      "{105}->{494,815}\n",
      "{208}->{888,290}\n",
      "{217,346}->{33,283}\n",
      "{458}->{208,969}\n",
      "{105}->{944,815}\n",
      "{208}->{969,290}\n",
      "{515}->{217,346}\n",
      "{819}->{684,70}\n",
      "{515}->{33,346}\n",
      "{33}->{283,515}\n",
      "{208}->{969,458}\n",
      "{704}->{825,39}\n",
      "{227,390}->{722,12}\n",
      "{208}->{888,969}\n",
      "{105}->{944,494}\n",
      "{208}->{458,290}\n",
      "{217,346}->{283,33,515}\n",
      "{208}->{458}\n",
      "{208}->{888,458}\n",
      "{33}->{283,217,346,515}\n",
      "{546}->{947,923}\n",
      "{515}->{217,346,283}\n",
      "{33}->{217,346,283}\n",
      "{33}->{283,346,515}\n",
      "{105}->{944,862}\n",
      "{33}->{515,217,283}\n",
      "{217,346}->{283,515}\n",
      "{33}->{217,346,515}\n",
      "{678}->{471}\n",
      "{515}->{33,346,283}\n",
      "{515}->{33,346,283,217}\n",
      "{33}->{217,283}\n",
      "{515}->{33,346,217}\n",
      "{33}->{346,283}\n",
      "{33}->{217,515}\n",
      "{515}->{217,283,33}\n",
      "{819}->{684,765}\n",
      "{546}->{947,923,661}\n",
      "{546}->{217,947,923,661}\n",
      "{546}->{217,661}\n",
      "{546}->{217,947,661}\n",
      "{33}->{346,515}\n",
      "{546}->{217,923}\n",
      "{546}->{217,923,947}\n",
      "{546}->{947,661}\n",
      "{546}->{217,923,661}\n",
      "{217,346}->{33,515}\n",
      "{515}->{217,283}\n",
      "{819}->{684,765,70}\n",
      "{678}->{960}\n",
      "{458}->{888,969,208}\n",
      "{208}->{969}\n",
      "{458}->{888,969,290,208}\n",
      "{722,390}->{227,12}\n",
      "{458}->{888,969,290}\n",
      "{458}->{208,969,290}\n",
      "{33}->{217,346}\n",
      "{458}->{888,290,208}\n",
      "{208}->{888}\n"
     ]
    }
   ],
   "source": [
    "Rules_multi_res = Rules_multi(data,candidates_1,frequent_itemsets, threshold_confidence)\n",
    "while(len(Rules_multi_res.to_inspect)>0):\n",
    "    Rules_multi_res.inspection(data,threshold_confidence)\n",
    "Rules_multi_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea16c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120  rules have been found.\n"
     ]
    }
   ],
   "source": [
    "print(len(Rules_multi_res.rules),\" rules have been found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec22ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rules_multi_res.verify(data,threshold_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7554d538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = True\n",
    "for a in list(Rules_res.rules):\n",
    "    res = res & Rules_multi_res.isinModel(a)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff70a046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rules_res.isinModel(list(Rules_multi_res.rules)[-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "15f3ceccf70e189784212a630bcb6f34ff17d0b5a012153ba4156d676d1715c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
